{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cathyai0320/A.I./blob/main/partb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**COMP 247 Supervised Learning**\n",
        "*   Group 2\n",
        "*   Part B\n",
        "*   Predict if incident would result in fatality or not"
      ],
      "metadata": {
        "id": "QAfuyk__2Uzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Members (alphabetically)**\n",
        "*   Boonluea, Chinnawut 301276464\n",
        "*   Chan, Kai Chung     301321990\n",
        "*   Mak, Chung Ping     301281670\n",
        "*   Pequino, Catherine  301308416\n",
        "*   Yurderi, Emre       301270260"
      ],
      "metadata": {
        "id": "I1H4-hhH2TWj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3s7iLYV2N_m"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import warnings\n",
        "import datetime as dt\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif, RFE\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler, FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import joblib\n",
        "import matplotlib.ticker as ticker\n",
        "from scipy.stats import chi2_contingency\n",
        "from imblearn.over_sampling import SMOTE, SMOTEN, SMOTENC\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# drop the rows without target value\n",
        "def drop_rows_without_target_value(data, target_column):\n",
        "    data.dropna(subset=[target_column], inplace=True)\n",
        "    return data"
      ],
      "metadata": {
        "id": "WF8VuvjbxFze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the age to mean of their respective class\n",
        "def transform_age(data, column):\n",
        "    age_mapping = {\n",
        "      '0 to 4': 2,\n",
        "      '5 to 9': 7,\n",
        "      '10 to 14': 12,\n",
        "      '15 to 19': 17,\n",
        "      '20 to 24': 22,\n",
        "      '25 to 29': 27,\n",
        "      '30 to 34': 32,\n",
        "      '35 to 39': 37,\n",
        "      '40 to 44': 42,\n",
        "      '45 to 49': 47,\n",
        "      '50 to 54': 52,\n",
        "      '55 to 59': 57,\n",
        "      '60 to 64': 62,\n",
        "      '65 to 69': 67,\n",
        "      '70 to 74': 72,\n",
        "      '75 to 79': 77,\n",
        "      '80 to 84': 82,\n",
        "      '85 to 89': 87,\n",
        "      '90 to 94': 92,\n",
        "      'Over 95': 97\n",
        "    }\n",
        "    data[column] = data[column].map(age_mapping)\n",
        "    data[column].fillna(data[column].mean(), inplace=True)\n",
        "    return data"
      ],
      "metadata": {
        "id": "ozup4ENLmcNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def impute_na_others(data, column):\n",
        "    data[column].fillna(\"others\", inplace=True)\n",
        "    return data"
      ],
      "metadata": {
        "id": "J8n-zYFBnae8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_fatality_ratio(df):\n",
        "    # Group by 'DISTRICT'\n",
        "    grouped = df.groupby('DISTRICT')\n",
        "\n",
        "    # Calculate the number of total accidents and fatal accidents\n",
        "    accident_counts = grouped['ACCLASS'].count()\n",
        "    fatal_counts = grouped['ACCLASS'].apply(lambda x: (x == 'Fatal').sum())\n",
        "\n",
        "    # Calculate the fatality rate\n",
        "    fatality_rate = fatal_counts / accident_counts\n",
        "\n",
        "    # Merge the calculated fatality rates back into the original DataFrame\n",
        "    df[\"FATALITY_RATE\"] = df['DISTRICT'].map(fatality_rate)\n",
        "    return df"
      ],
      "metadata": {
        "id": "FGnIB_DgwYLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_column(df, columns):\n",
        "    df = df[columns]\n",
        "    return df"
      ],
      "metadata": {
        "id": "wzgc2EwZxjEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function use LOCCOORD to fill the missing the values for ACCLOC, if LOCCOORD is also NA, then fill ACCLOC with \"Unknown\"\n",
        "def transform_loccord(data):\n",
        "    data['ACCLOC'] = data['ACCLOC'].fillna(data['LOCCOORD'])\n",
        "    data['ACCLOC'] = data['ACCLOC'].fillna(\"Unknown\")\n",
        "    return data"
      ],
      "metadata": {
        "id": "T-GuzJrNzu15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to map binary\n",
        "def transform_target_to_binary(data, target_column):\n",
        "    data[target_column] = data[target_column].map(lambda x: 1 if x == 'Fatal' else 0)\n",
        "    return data"
      ],
      "metadata": {
        "id": "mo9mJs3NJKlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert multiple columns to binary\n",
        "def transform_to_binary(data, columns):\n",
        "    for column in columns:\n",
        "        data[column] = data[column].fillna(\"No\")\n",
        "        data[column] = data[column].replace({'Yes': 1, 'No': 0})\n",
        "    return data"
      ],
      "metadata": {
        "id": "g0e0XGm2JUSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Encoding\n",
        "def label_encode(data, columns):\n",
        "    label_encoder = LabelEncoder()\n",
        "    for col in columns:\n",
        "        data[col] = label_encoder.fit_transform(data[col])\n",
        "    return data"
      ],
      "metadata": {
        "id": "hN0WYmvveCp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_pipeline = Pipeline([\n",
        "    ('drop_rows_without_target_value', FunctionTransformer(drop_rows_without_target_value, kw_args={'target_column': 'ACCLASS'})),\n",
        "    ('impute_na_others', FunctionTransformer(impute_na_others, kw_args={'column': 'DISTRICT'})),\n",
        "    ('calculate_fatality_ratio',FunctionTransformer(calculate_fatality_ratio)),\n",
        "    ('transform_loccord', FunctionTransformer(transform_loccord)),\n",
        "    ('transform_target_to_binary', FunctionTransformer(transform_target_to_binary, kw_args={'target_column': 'ACCLASS'})),\n",
        "    ('transform_to_binary', FunctionTransformer(transform_to_binary, kw_args={'columns': ['PEDESTRIAN', 'CYCLIST', 'TRUCK', 'TRSN_CITY_VEH', 'SPEEDING']})),\n",
        "    ('transform_age', FunctionTransformer(transform_age, kw_args={'column': 'INVAGE'})),\n",
        "    ('select_column', FunctionTransformer(select_column, kw_args={'columns': [\"ACCLASS\", \"INVAGE\", \"TRSN_CITY_VEH\", \"DISTRICT\", \"CYCLIST\", \"PEDESTRIAN\", \"SPEEDING\", \"ACCLOC\", \"TRAFFCTL\", \"TRUCK\", \"INVTYPE\", \"HOOD_158\", \"FATALITY_RATE\"]})),\n",
        "    ('column_impute', ColumnTransformer([\n",
        "      ('impute_others', SimpleImputer(strategy='constant',fill_value='others'), ['INVTYPE', 'TRAFFCTL']),\n",
        "      ('impute_most_frequent', SimpleImputer(strategy='most_frequent'), ['HOOD_158'])\n",
        "    ], remainder='passthrough')),\n",
        "    # ('label_encode', FunctionTransformer(label_encode, kw_args={'columns': [\n",
        "    #     \"HOOD_158\",\n",
        "    #     # \"INVTYPE\",\n",
        "    #     # \"DISTRICT\",\n",
        "    #     # \"TRAFFCTL\",\n",
        "    #     # \"ACCLOC\"\n",
        "    # ]}))\n",
        "])"
      ],
      "metadata": {
        "id": "bufw5BMFwzC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data into dataframe\n",
        "path = '/content/KSI.csv'\n",
        "df = pd.read_csv(path)"
      ],
      "metadata": {
        "id": "YL9ARfx1fj3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessed data\n",
        "preprocessed_data = input_pipeline.fit_transform(df)"
      ],
      "metadata": {
        "id": "bzBQ-OYvJaLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(preprocessed_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeEGMJYapPnq",
        "outputId": "a840861d-7963-4928-ce6f-33fce30d5399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Driver' 'Traffic Signal' '88' 1 42.40340179717587 0\n",
            " 'Toronto and East York' 0 1 0 'At Intersection' 0 0.10530612244897959]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert back to dataframe\n",
        "preprocessed_df = pd.DataFrame(\n",
        "    preprocessed_data,\n",
        "    columns=[\"INVTYPE\", \"TRAFFCTL\", \"HOOD_158\", \"ACCLASS\", \"INVAGE\", \"TRSN_CITY_VEH\", \"DISTRICT\", \"CYCLIST\", \"PEDESTRIAN\", \"SPEEDING\", \"ACCLOC\", \"TRUCK\", \"FATALITY_RATE\"]\n",
        ")"
      ],
      "metadata": {
        "id": "64AlHuKAMAvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# another processing\n",
        "preprocessed_df['INVAGE'] = preprocessed_df['INVAGE'].astype(float)\n",
        "preprocessed_df['FATALITY_RATE'] = preprocessed_df['FATALITY_RATE'].astype(float)\n",
        "\n",
        "\n",
        "encoded_features = [\"HOOD_158\",\"INVTYPE\",\"DISTRICT\",\"TRAFFCTL\",\"ACCLOC\"]\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "for column in encoded_features:\n",
        "    preprocessed_df[column] = label_encoder.fit_transform(preprocessed_df[column])\n",
        "\n",
        "preprocessed_df['ACCLASS'] = preprocessed_df['ACCLASS'].astype(int)\n",
        "preprocessed_df['PEDESTRIAN'] = preprocessed_df['PEDESTRIAN'].astype(int)\n",
        "preprocessed_df['CYCLIST'] = preprocessed_df['CYCLIST'].astype(int)\n",
        "preprocessed_df['TRUCK'] = preprocessed_df['TRUCK'].astype(int)\n",
        "preprocessed_df['TRSN_CITY_VEH'] = preprocessed_df['TRSN_CITY_VEH'].astype(int)\n",
        "preprocessed_df['SPEEDING'] = preprocessed_df['SPEEDING'].astype(int)"
      ],
      "metadata": {
        "id": "isKAwPWNNP-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = preprocessed_df[\"ACCLASS\"]\n",
        "X = preprocessed_df.drop(\"ACCLASS\", axis = 1)"
      ],
      "metadata": {
        "id": "hoYOHXLlV8Ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lXwSCWKo_85",
        "outputId": "9b38c9f7-cf6e-468e-bc85-2f5d2ec4e35a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 18189 entries, 0 to 18188\n",
            "Data columns (total 12 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   INVTYPE        18189 non-null  int64  \n",
            " 1   TRAFFCTL       18189 non-null  int64  \n",
            " 2   HOOD_158       18189 non-null  int64  \n",
            " 3   INVAGE         18189 non-null  float64\n",
            " 4   TRSN_CITY_VEH  18189 non-null  int64  \n",
            " 5   DISTRICT       18189 non-null  int64  \n",
            " 6   CYCLIST        18189 non-null  int64  \n",
            " 7   PEDESTRIAN     18189 non-null  int64  \n",
            " 8   SPEEDING       18189 non-null  int64  \n",
            " 9   ACCLOC         18189 non-null  int64  \n",
            " 10  TRUCK          18189 non-null  int64  \n",
            " 11  FATALITY_RATE  18189 non-null  float64\n",
            "dtypes: float64(2), int64(10)\n",
            "memory usage: 1.7 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Train Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y)"
      ],
      "metadata": {
        "id": "jyJowTPog3O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smote_numerical   = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "# smote_categorical = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "X_train_resampled, y_train_resampled  = smote_numerical.fit_resample(X_train.select_dtypes(include=['float64', 'int64']), y_train)\n",
        "# X_train_cat_resampled, y_train_cat_resampled          = smote_categorical.fit_resample(X_train.select_dtypes(include=['object']), y_train)"
      ],
      "metadata": {
        "id": "eaO9eJn-g4ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression and GridSearchCV\n",
        "param_grid_logistic = {\n",
        "    'C': [0.1, 1.0, 10.0],\n",
        "    'solver': ['lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "    'penalty': ['l1', 'l2', 'None']\n",
        "}\n",
        "\n",
        "logistic_regression_model = LogisticRegression()\n",
        "\n",
        "grid_search_logistic = GridSearchCV(logistic_regression_model, param_grid=param_grid_logistic, cv=5)\n",
        "grid_search_logistic.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "best_params_logistic = grid_search_logistic.best_params_\n",
        "print(\"Best Hyperparameters for Logistic Regression:\")\n",
        "print(best_params_logistic)\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_pred_logistic = grid_search_logistic.predict(X_test)\n",
        "accuracy_logistic = accuracy_score(y_test, y_pred_logistic)\n",
        "precision_logistic = precision_score(y_test, y_pred_logistic, average=\"macro\")*100\n",
        "recall_logistic = recall_score(y_test, y_pred_logistic, average=\"macro\")*100\n",
        "print(f\"Logistic Regression Accuracy: {accuracy_logistic:.4f}\")\n",
        "print(f\"Logistic Regression Precision: {precision_logistic:.4f}\")\n",
        "print(f\"Logistic Regression Recall: {recall_logistic:.4f}\")\n",
        "print('Confusion Matrix: ')\n",
        "confusion_matrix(y_test, y_pred_logistic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q45b2myMhiJJ",
        "outputId": "62f11ba4-67e6-4bac-9e5c-5ec4ce80a1c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters for Logistic Regression:\n",
            "{'C': 1.0, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Logistic Regression Accuracy: 0.5313\n",
            "Logistic Regression Precision: 50.6736\n",
            "Logistic Regression Recall: 51.3792\n",
            "Confusion Matrix: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1681, 1442],\n",
              "       [ 263,  252]])"
            ]
          },
          "metadata": {},
          "execution_count": 271
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dump Logistic Regression model with best parameters\n",
        "best_logistic_model = grid_search_logistic.best_estimator_\n",
        "best_logistic_params = grid_search_logistic.best_params_\n",
        "joblib.dump(best_logistic_model, 'best_logistic_model.h5')\n",
        "joblib.dump(best_logistic_params, 'best_logistic_params.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5iy691OqBkB",
        "outputId": "0b8a5b81-ea26-4aa3-ac7b-ef55b23db74e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['best_logistic_params.h5']"
            ]
          },
          "metadata": {},
          "execution_count": 272
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree model\n",
        "decision_tree_model = DecisionTreeClassifier()\n",
        "\n",
        "param_grid = {\n",
        "    'criterion': ['gini', 'entropy'], # The function to measure the quality of a split\n",
        "    'splitter': ['best', 'random'],   # The strategy used to choose the split at each node\n",
        "    'max_depth': np.arange(1, 35),    # The maximum depth of the tree\n",
        "    'min_samples_split': np.arange(2, 25),    # The minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': np.arange(1, 30),     # The minimum number of samples required to be at a leaf node\n",
        "    'max_features': ['auto', 'sqrt', 'log2', None]    # The number of features to consider when looking for the best split\n",
        "}\n",
        "\n",
        "randomized_search_decision_tree = RandomizedSearchCV(decision_tree_model, param_distributions=param_grid, n_iter=100, cv=5)\n",
        "randomized_search_decision_tree.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "best_params_randomized_decision_tree = randomized_search_decision_tree.best_params_\n",
        "print(\"Best Parameters for Decision Tree (Randomized Search):\")\n",
        "print(best_params_randomized_decision_tree)\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_pred_randomized_decision_tree = randomized_search_decision_tree.predict(X_test)\n",
        "accuracy_randomized_decision_tree = accuracy_score(y_test, y_pred_randomized_decision_tree)\n",
        "precision_randomized_decision_tree = precision_score(y_test, y_pred_randomized_decision_tree, average=\"macro\")*100\n",
        "recall_randomized_decision_tree = recall_score(y_test, y_pred_randomized_decision_tree, average=\"macro\")*100\n",
        "print(f\"Randomized Decision Tree Accuracy: {accuracy_randomized_decision_tree:.4f}\")\n",
        "print(f\"Randomized Decision Tree Precision: {precision_randomized_decision_tree:.4f}\")\n",
        "print(f\"Randomized Decision Tree Recall: {recall_randomized_decision_tree:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BawmYK_qH3Q",
        "outputId": "ea2ef240-1ae2-4c69-e6cf-50ffbd9dd557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters for Decision Tree (Randomized Search):\n",
            "{'splitter': 'best', 'min_samples_split': 11, 'min_samples_leaf': 1, 'max_features': None, 'max_depth': 24, 'criterion': 'entropy'}\n",
            "Randomized Decision Tree Accuracy: 0.7883\n",
            "Randomized Decision Tree Precision: 59.9442\n",
            "Randomized Decision Tree Recall: 62.0517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dump Decision Tree model with best parameters\n",
        "best_decision_tree_model = randomized_search_decision_tree.best_estimator_\n",
        "best_decision_tree_params = randomized_search_decision_tree.best_params_\n",
        "joblib.dump(best_decision_tree_model, 'best_decision_tree_model.h5')\n",
        "joblib.dump(best_decision_tree_params, 'best_decision_tree_params.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdJQs_iFqNJQ",
        "outputId": "c8ab93a0-31f1-41e4-9d84-3cf337bfd7f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['best_decision_tree_params.h5']"
            ]
          },
          "metadata": {},
          "execution_count": 274
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Random Forest model\n",
        "random_forest_model = RandomForestClassifier()\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': np.arange(100, 1001, 250),    # Number of trees in the forest\n",
        "    'criterion': ['gini', 'entropy'],             # The function to measure the quality of a split\n",
        "    'max_depth': np.arange(1, 35),                # The maximum depth of the trees\n",
        "    'min_samples_split': np.arange(2, 30),        # The minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': np.arange(1, 21),         # The minimum number of samples required to be at a leaf node\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],     # The number of features to consider when looking for the best split\n",
        "    'bootstrap': [True, False]                    # Whether bootstrap samples are used when building trees\n",
        "}\n",
        "\n",
        "# Randomized Search\n",
        "randomized_search_random_forest = RandomizedSearchCV(random_forest_model, param_distributions=param_grid, n_iter=100, cv=5)\n",
        "randomized_search_random_forest.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "best_params_randomized_random_forest = randomized_search_random_forest.best_params_\n",
        "print(\"Best Parameters for Random Forest (Randomized Search):\")\n",
        "print(best_params_randomized_random_forest)\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_pred_randomized_random_forest = randomized_search_random_forest.predict(X_test)\n",
        "accuracy_randomized_random_forest = accuracy_score(y_test, y_pred_randomized_random_forest)\n",
        "precision_randomized_random_forest = precision_score(y_test, y_pred_randomized_random_forest, average=\"macro\")*100\n",
        "recall_randomized_random_forest = recall_score(y_test, y_pred_randomized_random_forest, average=\"macro\")*100\n",
        "print(f\"Random Forest Accuracy (Randomized Search): {accuracy_randomized_random_forest:.4f}\")\n",
        "print(f\"Random Forest Precision (Randomized Search): {precision_randomized_random_forest:.4f}\")\n",
        "print(f\"Random Forest Recall (Randomized Search): {recall_randomized_random_forest:.4f}\")"
      ],
      "metadata": {
        "id": "NMbE8MOzqXnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dump Random Forest model with best parameters\n",
        "best_random_forest_model = randomized_search_random_forest.best_estimator_\n",
        "best_random_forest_params = randomized_search_random_forest.best_params_\n",
        "joblib.dump(best_random_forest_model, 'best_random_forest_model.h5')\n",
        "joblib.dump(best_random_forest_params, 'best_random_forest_params.h5')"
      ],
      "metadata": {
        "id": "sMU2sMviqd0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def send_slack_notification(webhook_url, message):\n",
        "    payload = {\n",
        "        \"text\": message\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    response = requests.post(webhook_url, data=json.dumps(payload), headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        print(\"Notification sent to Slack successfully.\")\n",
        "    else:\n",
        "        print(f\"Failed to send notification to Slack. Status code: {response.status_code}\")\n",
        "\n",
        "# Replace this with your actual webhook URL\n",
        "slack_webhook_url = \"https://hooks.slack.com/services/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\""
      ],
      "metadata": {
        "id": "QLmXkVdCt4y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "send_slack_notification(slack_webhook_url, \"Random Forest model model training has finished.\")"
      ],
      "metadata": {
        "id": "83o8ounft9iD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Neural Network model\n",
        "neural_network_model = MLPClassifier()\n",
        "param_grid_neural_network = {\n",
        "    'hidden_layer_sizes': [(2), (3), (3,5)],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'solver': ['adam', 'sgd'],\n",
        "    'alpha': [0.0001, 0.001, 0.01],\n",
        "    'learning_rate': ['constant', 'invscaling', 'adaptive']\n",
        "}\n",
        "\n",
        "grid_search_neural_network = GridSearchCV(neural_network_model, param_grid=param_grid_neural_network, cv=5)\n",
        "grid_search_neural_network.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "best_params_grid_neural_network = grid_search_neural_network.best_params_\n",
        "print(\"Best Parameters for Neural Network (Grid Search):\")\n",
        "print(best_params_grid_neural_network)\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_pred_neural_network = grid_search_neural_network.predict(X_test)\n",
        "accuracy_neural_network = accuracy_score(y_test, y_pred_neural_network)\n",
        "precision_neural_network = precision_score(y_test, y_pred_neural_network, average=\"macro\")*100\n",
        "recall_neural_network = recall_score(y_test, y_pred_neural_network, average=\"macro\")*100\n",
        "print(f\"Neural Network Accuracy (Grid Search): {accuracy_neural_network:.4f}\")\n",
        "print(f\"Neural Network Precision (Grid Search): {precision_neural_network:.4f}\")\n",
        "print(f\"Neural Network Recall (Grid Search): {recall_neural_network:.4f}\")\n"
      ],
      "metadata": {
        "id": "ZZxjMj1kq6Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dump Neural Network model with best parameters\n",
        "best_neural_network_model = grid_search_neural_network.best_estimator_\n",
        "best_neural_network_params = grid_search_neural_network.best_params_\n",
        "joblib.dump(best_neural_network_model, 'best_neural_network_model.h5')\n",
        "joblib.dump(best_neural_network_params, 'best_neural_network_params.h5')"
      ],
      "metadata": {
        "id": "_zL3yr9Wq7yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "send_slack_notification(slack_webhook_url, \"Neural Network model model training has finished.\")"
      ],
      "metadata": {
        "id": "4arfANl7uBH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "xgb_pipeline = Pipeline([\n",
        "                (\"xgb_clf\",XGBClassifier(random_state=42))\n",
        "    ])\n",
        "\n",
        "parameters={\n",
        "    'xgb_clf__learning_rate': stats.uniform(0.01, 0.3),\n",
        "    'xgb_clf__n_estimators': [50, 100, 150, 200],\n",
        "    'xgb_clf__max_depth': [3, 5, 7, 9],\n",
        "    'xgb_clf__min_child_weight': [1, 5, 10],\n",
        "    'xgb_clf__gamma': [0, 0.1, 0.2, 0.3],\n",
        "    'xgb_clf__subsample': [0.8, 0.9, 1.0],\n",
        "    'xgb_clf__colsample_bytree': [0.8, 0.9, 1.0],\n",
        "    'xgb_clf__colsample_bylevel': [0.8, 0.9, 1.0],\n",
        "    'xgb_clf__reg_alpha': [0, 0.001, 0.01, 0.1, 1.0],\n",
        "    'xgb_clf__reg_lambda': [0, 0.001, 0.01, 0.1, 1.0],\n",
        "    'xgb_clf__scale_pos_weight': [1, 2, 5, 10],\n",
        "    'xgb_clf__eval_metric': ['logloss']\n",
        "}\n",
        "xgb_grid_pipeline=RandomizedSearchCV(xgb_pipeline, parameters,n_jobs=-1, cv=5, n_iter=100, refit='str',random_state=42)\n",
        "\n",
        "xgb_grid_pipeline.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "best_params_randomized_xgboost = xgb_grid_pipeline.best_params_\n",
        "print(\"Best Parameters for XGBoost (Randomized Search):\")\n",
        "print(best_params_randomized_xgboost)\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_pred_xgboost = xgb_grid_pipeline.predict(X_test)\n",
        "accuracy_xgboost = accuracy_score(y_test, y_pred_xgboost)\n",
        "precision_xgboost = precision_score(y_test, y_pred_xgboost, average=\"macro\")*100\n",
        "recall_xgboost = recall_score(y_test, y_pred_xgboost, average=\"macro\")*100\n",
        "print(f\"XGBoost Accuracy (Randomized Search): {accuracy_xgboost:.4f}\")\n",
        "print(f\"XGBoost Precision (Randomized Search): {precision_xgboost:.4f}\")\n",
        "print(f\"XGBoost Recall (Randomized Search): {recall_xgboost:.4f}\")"
      ],
      "metadata": {
        "id": "piGve8IOq9XC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dump XGBoost model with best parameters\n",
        "best_xgboost_model = xgb_grid_pipeline.best_estimator_\n",
        "best_xgboost_params = xgb_grid_pipeline.best_params_\n",
        "joblib.dump(best_xgboost_model, 'best_xgboost_model.h5')\n",
        "joblib.dump(best_xgboost_params, 'best_xgboost_params.h5')"
      ],
      "metadata": {
        "id": "P78CmXR4q_BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "send_slack_notification(slack_webhook_url, \"XGBoost model model training has finished.\")"
      ],
      "metadata": {
        "id": "OasGhD0fuD7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the SVM model\n",
        "svm_model = SVC()\n",
        "\n",
        "param_grid_svm = {\n",
        "    'C': [0.1, 1.0],\n",
        "    'kernel': ['linear', 'poly', 'sigmoid'],\n",
        "}\n",
        "\n",
        "randomized_search_svm = RandomizedSearchCV(svm_model, param_distributions=param_grid_svm, n_iter=50, cv=5)\n",
        "randomized_search_svm.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "best_params_randomized_svm = randomized_search_svm.best_params_\n",
        "print(\"Best Parameters for SVM (Randomized Search):\")\n",
        "print(best_params_randomized_svm)\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_pred_svm = randomized_search_svm.predict(X_test)\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "precision_svm = precision_score(y_test, y_pred_svm, average=\"macro\")*100\n",
        "recall_svm = recall_score(y_test, y_pred_svm, average=\"macro\")*100\n",
        "print(f\"SVC Accuracy (Randomized Search): {accuracy_svm:.4f}\")\n",
        "print(f\"SVC Precision (Randomized Search): {precision_svm:.4f}\")\n",
        "print(f\"SVC Recall (Randomized Search): {recall_svm:.4f}\")\n"
      ],
      "metadata": {
        "id": "e8qEdGgzrBNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dump SVM model with best parameters\n",
        "best_svm_model = randomized_search_svm.best_estimator_\n",
        "best_svm_params = randomized_search_svm.best_params_\n",
        "joblib.dump(best_svm_model, 'best_svm_model.h5')\n",
        "joblib.dump(best_svm_params, 'best_svm_params.h5')"
      ],
      "metadata": {
        "id": "TpwmUgz8rDYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "send_slack_notification(slack_webhook_url, \"SVM model model training has finished.\")"
      ],
      "metadata": {
        "id": "9RqqqnscuGH5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}